---
title: "vocab composition"
author: "Samah"
date: "2024-05-05"
output: html_document
---

To do items:

- make table 1 from Samuelson and Smith


# Preprocessing 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(purrr)
library(ggrepel)
library(here)
library(irr)
library(ggvenn)
library(gridExtra)
source(here("..","helper.R"))
```

Read in correct data. 

```{r}
# pilot_data = read_csv("samah.varlogs.pilot.csv")
# full_data1 = read_csv("samah.varlogs.csv")
full_data = read_csv("redo.csv")
```

Apply the helper function to the jsonData column

```{r}
full_data$word <- full_data$theword
full_data$response <- sapply(full_data$response, extractValue)

full_data <- full_data %>% filter(trial_type == "survey-multi-choice")
full_data <- full_data %>%
  filter(!str_detect(block, "practice") & !str_detect(block, "attention") ) %>% 
  filter(!is.na(word))

print(paste("mean number of ratings per word"))

full_data %>% group_by(word, block) %>% summarize( count = n()) %>% ungroup() %>% summarise(mean = sum(count)/ n())

print(paste("mean number of ratings per word"))

full_data %>% group_by(word, block) %>% summarize( count = n()) %>% ungroup() %>% summarise(mean = sum(count)/ n())

```

Read in language data. 

```{r}
word_language_mappings <- read_csv("alllangs.csv") %>% 
  filter(!is.na(uni_lemma))  %>% 
  mutate(word = uni_lemma) %>%
  select(word, language)
```
Define mapping dictionaries
 
```{r }
# Apply the mapping function in helper.R to the response column
full_data$response_numeric <- mapply(map_categories, full_data$response, full_data$block)
```


# Data processing 

Proportions

``` {r}

full_data <- full_data %>% group_by(word, block) %>%
  mutate(totcount = n()) %>% ungroup()

# how many ratings per response per block
category_counts <- full_data %>%
  group_by(word, block, response, totcount) %>%
  summarise(count = n(), .groups = 'drop') 

prop_shape <- category_counts %>% filter(response == "shape")


category_counts <- category_counts %>%
  mutate(proportion = count/totcount)

#languages = word_language_mappings
lang_df <- full_join(category_counts, word_language_mappings) %>% 
  filter( !is.na(word), !is.na(language))
```

Divide the lang_df by block. shared words amongst (solidity, syntax, and category) based on proportion.

```{r }
solid_df <- lang_df %>% filter(response == "solid")
count_df <- lang_df %>% filter(response == "count noun")
shape_df <- lang_df %>% filter(response == "shape")
```


# Distribution plots

Within category task:

```{r}
lang_df |>
  filter(block == "category_organization") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```
```{r}
lang_df |>
  filter(block == "count_mass") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```
```{r}
lang_df |>
  filter(block == "solidity") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```

Create combined density plot.

``` {r}
lang_df |>
  filter(response == "solid" | response == "count noun" | 
           response == "shape") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```


# Investigation of why shape bias is low!

Languages here don't matter - so we use `category_counts`. 

```{r}
category_counts |>
  filter(response == "shape") |>
  arrange(desc(proportion))
```




# Inter-rater agreement

binomial and agreemment among raters

``` {r}
# Group by 'word' and 'block'

#full_datas <- full_data %>% select(word, subject, response, block)
full_datas <- full_data %>% select(word, subject, response, block)


grouped_data <- full_datas %>%
  group_by(word, block) %>%
  summarise(response_count = n(),
            most_common_response = max(table(response)),
            total_responses = n(),
            .groups = 'drop')

# proportion of agreement for most common response
grouped_data <- grouped_data %>%
  mutate(proportion_agreement = most_common_response / total_responses)

# Function to perform binomial test
perform_binom_test <- function(successes, total) {
  binom.test(successes, total, p = 0.7, alternative = "greater")
}


results <- grouped_data %>%
  rowwise() %>%
  mutate(binom_test = list(perform_binom_test(most_common_response, total_responses)),
         p_value = binom_test$p.value,
         significant = p_value < 0.05) %>%
  select(word, block, proportion_agreement, p_value, significant)


print(results)

agreement <- grouped_data %>%
  group_by(block) %>%
  summarise(m = mean(proportion_agreement),
            se = sd(proportion_agreement) / sqrt(n()))

ggplot(data = agreement, aes(x = block, y = m)) +
  geom_col(fill = "skyblue", width = 0.5) +  # Narrow bars
  geom_errorbar(aes(ymin = m - se, ymax = m + se), width = 0.2) + 
  labs(x = "Block", y = "Proportion of Agreement", title = "Mean Proportion of Agreement by Block") +
  theme_minimal() +
  theme(aspect.ratio = 0.5) 

# specific dimensions
ggsave("agreement_plot.png", width = 5, height = 4)  

```

Cohen's Kappa: inter rater agreement:
Kappa.light is not working because we have a lot of NAs

``` {r }
solid_kappa <- full_data %>% filter(block == "solidity") 
count_kappa <- full_data %>% filter(block == "count_mass")
shape_kappa <- full_data %>% filter(block == "category_organization")

# %>% select(word, response_numeric, subject ) %>% pivot_wider(names_from = subject, values_from = response_numeric)


ratings_wide <- solid_kappa %>%
  select(word, response_numeric, subject) %>%
  pivot_wider(names_from = subject, values_from = response_numeric)

# matrix is compatible with irr package functions) ?
ratings_matrix <- as.matrix(ratings_wide %>% select(-word))

# kappam.light() can handle NAs ?
kappa_result <- kappam.light(ratings_matrix)
print(kappa_result)

```

using pairwise cohen's kappa: 
Kappa pairwise is not informative because each rater is getting a different set of words, so the kappa score is calculated based on a few words shared amongs raters. which could explain why we are getting many zero agreement scores ? 

``` {r }

calculate_kappa <- function(data, block_type) {
  block_data <- data %>% filter(block == block_type)
  
  ratings_wide <- block_data %>%
    select(word, response_numeric, subject) %>%
    pivot_wider(names_from = subject, values_from = response_numeric)
  #unique pairs of subjects
  raters <- colnames(ratings_wide)[-1]  # Exclude "word"
  pairs <- combn(raters, 2, simplify = FALSE)
  

  pairwise_kappas <- sapply(pairs, function(pair) {
    ratings_pair <- ratings_wide %>%
      select(word, all_of(pair)) %>%
      drop_na()
    
    if (nrow(ratings_pair) > 1) {
      suppressWarnings(kappa2(as.matrix(ratings_pair[, -1]), weight = "unweighted")$value)
    } else {
      NA
    }
  })
  
  mean_kappa <- mean(pairwise_kappas, na.rm = TRUE)
  print(paste("Mean Kappa for", block_type, "block:", mean_kappa))
  
  #Kappa distribution
  pairwise_results <- data.frame(
    Pair = sapply(pairs, function(x) paste(x, collapse = "-")),
    Kappa = pairwise_kappas
  )
  
  ggplot(pairwise_results, aes(x = Kappa)) +
    geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black") +
    labs(title = paste("Distribution of Pairwise Kappa Scores -", block_type, "Block"), 
         x = "Kappa", y = "Frequency") +
    theme_minimal()
}


calculate_kappa(full_data, "solidity")
calculate_kappa(full_data, "count_mass")
calculate_kappa(full_data, "category_organization")

```
 
 only solid
``` {r }

ratings_wide1 <- solid_kappa %>%
  select(word, response_numeric, subject) %>%
  pivot_wider(names_from = subject, values_from = response_numeric)

#create all unique pairs of raters
raters <- colnames(ratings_wide1)[-1]  # exclude "word" 
pairs <- combn(raters, 2, simplify = FALSE)

# calculate pairwise Cohen's Kappa
pairwise_kappas <- sapply(pairs, function(pair) {
  ratings_pair <- ratings_wide1 %>%
    select(word, all_of(pair)) %>%
    drop_na()
  
  #are there enough ratings to calculate kappa?
  if (nrow(ratings_pair) > 1) {
    kappa2(as.matrix(ratings_pair[, -1]), weight = "unweighted")$value
  } else {
    NA 
  }
})


mean_kappa <- mean(pairwise_kappas, na.rm = TRUE)
print(mean_kappa)

```

visualizing Kappa values: only solid
``` {r }

pairwise_results <- data.frame(
  Pair = sapply(pairs, function(x) paste(x, collapse = "-")),
  Kappa = pairwise_kappas
)
print(pairwise_results)


library(ggplot2)
ggplot(pairwise_results, aes(x = Kappa)) +
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Pairwise Kappa Scores", x = "Kappa", y = "Frequency") +
  theme_minimal()

```

# Venn diagram

Try to get a venn diagram-type representation. 

```{r}
threshold <- .5

category_thresholds <- category_counts |>
  filter(response %in% c("solid","count noun","shape")) |>
  mutate(over_threshold = proportion > threshold) |>
  select(-block, -totcount, -count, -proportion) |> 
  pivot_wider(names_from = "response", values_from = "over_threshold", 
              values_fill = FALSE) |>
  rename(count_noun = `count noun`)

category_overlap <- word_language_mappings |>
  left_join(category_thresholds) |>
  filter(!is.na(count_noun)) 
```

One example. 

```{r}
eng <- filter(category_overlap, language == "English (American)") |>
  select(-language) |>
  filter(!is.na(count_noun))
ggvenn(eng, c("count_noun","solid","shape"))
```
Lots of plots. 

```{r}
venns <- category_overlap |>
  group_by(language) |>
  nest() |>
  mutate(plot = map(data, ~ggvenn(.x, c("count_noun","solid","shape"),
                                  show_percentage = FALSE, 
                                  text_size = 3,
                                  set_name_size = 3)
                    + ggtitle(language))) |>
  pull(plot)

grid.arrange(grobs = venns, ncol = 4)
```








```{r}
inner_join(solid_df %>% filter(proportion <= 0.5), 
           count_df %>% filter(proportion <= 0.5), 
           by= "word") %>% 
  select(word) %>% 
  unique()

inner_join(solid_df %>% filter(proportion > 0.5), 
           count_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), count_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), count_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), shape_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), shape_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), shape_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), shape_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()
```



``` {r }
dfs <- list(solid_df, count_df, shape_df)
names <- c("solid", "count", "shape")
conditions <- c("low", "high")

# Generate all unique pairs of dataframe and condition combinations
comparison_combinations <- expand.grid(
  df1 = names, df2 = names,
  prop1 = conditions, prop2 = conditions,
  stringsAsFactors = FALSE
) %>%
  filter(df1 != df2)  # no same dataframe comparisons

# Apply over each combination
results <- comparison_combinations %>%
  pmap_dfr(~ find_shared_words(
    dfs[[which(names == ..1)]],
    dfs[[which(names == ..2)]],
    ..1, ..2, ..3, ..4
  ))
 print(results)

results %>% group_by(source1, source2, condition1, condition2) %>% summarize(count = n())
```

randomly sample 50 words: 

``` {r}
randm_solid <- category_counts %>%
  filter(block == "solidity") %>% 
  {                               
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }

randm_syntax <- category_counts %>% filter(block == "count_mass") %>% 
  {
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }

randm_categ <- category_counts %>% filter(block == "category_organization") %>% 
  {
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }
```
reorder

```{r}

solid_proportions <- randm_solid %>%
  filter(response == "solid") %>%
  select(word, solid_proportion = proportion)

# merge the proportion of "solid" back into the main data frame
randm_solid <- randm_solid %>%
  left_join(solid_proportions, by = "word") %>%
  mutate(solid_proportion = replace_na(solid_proportion, 0))

# reorder 'word' based on 'solid_proportion'
randm_solid <- randm_solid %>% 
  mutate(word = fct_reorder(word, solid_proportion, .desc = TRUE))

#length(unique(languages$word))

```
```{r}
ggplot(randm_solid, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()

ggplot(randm_syntax, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()

ggplot(randm_categ, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()
```

ordered

```{r}

ggplot(randm_solid, aes(x = word, y = proportion, fill = response)) +
  geom_bar(position = "stack", stat = "identity") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
# rotate x-axis text for better readability

```