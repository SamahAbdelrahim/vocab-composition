---
title: "vocab composition"
author: "Samah"
date: "2024-05-05"
output: html_document
---

To do items:

- make table 1 from Samuelson and Smith


# Preprocessing 

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(purrr)
library(ggrepel)
library(here)
library(irr)
library(ggvenn)
library(gridExtra)
library(psych)
source(here("..","helper.R"))
```

Read in correct data. 

```{r}
# pilot_data = read_csv("samah.varlogs.pilot.csv")
# full_data1 = read_csv("samah.varlogs.csv")
full_data = read_csv(here("data", "redo.csv"))
```

Apply the helper function to the jsonData column

```{r}
full_data$word <- full_data$theword
full_data$response <- sapply(full_data$response, extractValue)

full_data <- full_data %>% filter(trial_type == "survey-multi-choice")
full_data <- full_data %>%
  filter(!str_detect(block, "practice") & !str_detect(block, "attention") ) %>% 
  filter(!is.na(word))

print(paste("mean number of ratings per word"))

full_data %>% group_by(word, block) %>% summarize( count = n()) %>% ungroup() %>% summarise(mean = sum(count)/ n())

print(paste("mean number of ratings per word"))

full_data %>% group_by(word, block) %>% summarize( count = n()) %>% ungroup() %>% summarise(mean = sum(count)/ n())

```

Read in language data. 

```{r}
word_language_mappings <- read_csv(here("data", "alllangs.csv")) %>% 
  filter(!is.na(uni_lemma))  %>% 
  mutate(word = uni_lemma) %>%
  select(word, language)
```
Define mapping dictionaries
 
```{r }
# Apply the mapping function in helper.R to the response column
full_data$response_numeric <- mapply(map_categories, full_data$response, full_data$block)
```


# Data processing 

Proportions

``` {r}

full_data <- full_data %>% group_by(word, block) %>%
  mutate(totcount = n()) %>% ungroup()

# how many ratings per response per block
category_counts <- full_data %>%
  group_by(word, block, response, totcount) %>%
  summarise(count = n(), .groups = 'drop') 

prop_shape <- category_counts %>% filter(response == "shape")


category_counts <- category_counts %>%
  mutate(proportion = count/totcount)

#languages = word_language_mappings
lang_df <- full_join(category_counts, word_language_mappings) %>% 
  filter( !is.na(word), !is.na(language))
```

Divide the lang_df by block. shared words amongst (solidity, syntax, and category) based on proportion.

```{r }
solid_df <- lang_df %>% filter(response == "solid")
count_df <- lang_df %>% filter(response == "count noun")
shape_df <- lang_df %>% filter(response == "shape")

meansolid_perlanguage <- solid_df %>% group_by(language) %>% summarize(estimates = mean(proportion))
```

# Distribution plots

Within category task:

```{r}
lang_df |>
  filter(block == "category_organization") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```
```{r}
lang_df |>
  filter(block == "count_mass") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```
```{r}
lang_df |>
  filter(block == "solidity") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density (words)"
  ) +
  theme_minimal()
```

Create combined density plot.

``` {r}
lang_df |>
  filter(response == "solid" | response == "count noun" | 
           response == "shape") |>
ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  geom_vline(xintercept = mean(lang_df$proportion), color = "red", linetype = "dashed") +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density"
  ) +
  theme_minimal()


mean_proportions <- lang_df |>
  filter(response %in% c("solid", "count noun", "shape")) |>
  group_by(language, response) |>
  summarize(mean_proportion = mean(proportion, na.rm = TRUE), groups = "drop")

dist <- lang_df |>
  filter(response %in% c("solid", "count noun", "shape")) |>
  ggplot(aes(x = proportion, color = response, fill = response)) +
  geom_density(alpha = 0.5) +
  geom_vline(data = mean_proportions, aes(xintercept = mean_proportion, color = response), 
            linetype = "dashed", size = 0.3) +
  facet_wrap(~ language) +
  labs(
    title = "Density of Proportions by Language",
    x = "Proportion Value",
    y = "Density"
  ) +
  theme_minimal()

ggsave("distribution.png", dist, width = 12, height = 8, dpi = 300)

```


# Investigation of why shape bias is low!

Languages here don't matter - so we use `category_counts`. 

```{r}
category_counts |>
  filter(response == "shape") |>
  arrange(desc(proportion))
```




# Inter-rater agreement

binomial and agreemment among raters

``` {r}
# Group by 'word' and 'block'

#full_datas <- full_data %>% select(word, subject, response, block)
full_datas <- full_data %>% select(word, subject, response, block)


grouped_data <- full_datas %>%
  group_by(word, block) %>%
  summarise(response_count = n(),
            most_common_response = max(table(response)),
            total_responses = n(),
            .groups = 'drop')

# proportion of agreement for most common response
grouped_data <- grouped_data %>%
  mutate(proportion_agreement = most_common_response / total_responses)

# Function to perform binomial test
perform_binom_test <- function(successes, total) {
  binom.test(successes, total, p = 0.7, alternative = "greater")
}


results <- grouped_data %>%
  rowwise() %>%
  mutate(binom_test = list(perform_binom_test(most_common_response, total_responses)),
         p_value = binom_test$p.value,
         significant = p_value < 0.05) %>%
  select(word, block, proportion_agreement, p_value, significant)


print(results)

agreement <- grouped_data %>%
  group_by(block) %>%
  summarise(m = mean(proportion_agreement),
            se = sd(proportion_agreement) / sqrt(n()))

ggplot(data = agreement, aes(x = block, y = m)) +
  geom_col(fill = "skyblue", width = 0.5) +  # Narrow bars
  geom_errorbar(aes(ymin = m - se, ymax = m + se), width = 0.2) + 
  labs(x = "Block", y = "Proportion of Agreement", title = "Mean Proportion of Agreement by Block") +
  theme_minimal() +
  theme(aspect.ratio = 0.5) 

# specific dimensions
ggsave("agreement_plot.png", width = 5, height = 4)  

```

Cohen's Kappa: inter rater agreement:
Kappa.light is not working because we have a lot of NAs

``` {r }
solid_kappa <- full_data %>% filter(block == "solidity") 
count_kappa <- full_data %>% filter(block == "count_mass")
shape_kappa <- full_data %>% filter(block == "category_organization")

# %>% select(word, response_numeric, subject ) %>% pivot_wider(names_from = subject, values_from = response_numeric)


ratings_wide <- solid_kappa %>%
  select(word, response_numeric, subject) %>%
  pivot_wider(names_from = subject, values_from = response_numeric)

# matrix is compatible with irr package functions) ?
ratings_matrix <- as.matrix(ratings_wide %>% select(-word))

# kappam.light() can handle NAs ?
kappa_result <- kappam.light(ratings_matrix)
print(kappa_result)

```

using pairwise cohen's kappa: 
Kappa pairwise is not informative because each rater is getting a different set of words, so the kappa score is calculated based on a few words shared amongs raters. which could explain why we are getting many zero agreement scores ? 

``` {r }

calculate_kappa <- function(data, block_type) {
  block_data <- data %>% filter(block == block_type)
  
  ratings_wide <- block_data %>%
    select(word, response_numeric, subject) %>%
    pivot_wider(names_from = subject, values_from = response_numeric)
  #unique pairs of subjects
  raters <- colnames(ratings_wide)[-1]  # Exclude "word"
  pairs <- combn(raters, 2, simplify = FALSE)
  

  pairwise_kappas <- sapply(pairs, function(pair) {
    ratings_pair <- ratings_wide %>%
      select(word, all_of(pair)) %>%
      drop_na()
    
    if (nrow(ratings_pair) > 1) {
      suppressWarnings(kappa2(as.matrix(ratings_pair[, -1]), weight = "unweighted")$value)
    } else {
      NA
    }
  })
  
  mean_kappa <- mean(pairwise_kappas, na.rm = TRUE)
  print(paste("Mean Kappa for", block_type, "block:", mean_kappa))
  
  #Kappa distribution
  pairwise_results <- data.frame(
    Pair = sapply(pairs, function(x) paste(x, collapse = "-")),
    Kappa = pairwise_kappas
  )
  
  ggplot(pairwise_results, aes(x = Kappa)) +
    geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black") +
    labs(title = paste("Distribution of Pairwise Kappa Scores -", block_type, "Block"), 
         x = "Kappa", y = "Frequency") +
    theme_minimal()
}


calculate_kappa(full_data, "solidity")
calculate_kappa(full_data, "count_mass")
calculate_kappa(full_data, "category_organization")

```
 
 only solid
``` {r }

ratings_wide1 <- solid_kappa %>%
  select(word, response_numeric, subject) %>%
  pivot_wider(names_from = subject, values_from = response_numeric)

#create all unique pairs of raters
raters <- colnames(ratings_wide1)[-1]  # exclude "word" 
pairs <- combn(raters, 2, simplify = FALSE)

# calculate pairwise Cohen's Kappa
pairwise_kappas <- sapply(pairs, function(pair) {
  ratings_pair <- ratings_wide1 %>%
    select(word, all_of(pair)) %>%
    drop_na()
  
  #are there enough ratings to calculate kappa?
  if (nrow(ratings_pair) > 1) {
    suppressWarnings(kappa2(as.matrix(ratings_pair[, -1]), weight = "unweighted")$value)
  } else {
    NA 
  }
})


mean_kappa <- mean(pairwise_kappas, na.rm = TRUE)
print(mean_kappa)

```

visualizing Kappa values: only solid
``` {r }

pairwise_results <- data.frame(
  Pair = sapply(pairs, function(x) paste(x, collapse = "-")),
  Kappa = pairwise_kappas
)
print(pairwise_results)


library(ggplot2)
ggplot(pairwise_results, aes(x = Kappa)) +
  geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black") +
  labs(title = "Distribution of Pairwise Kappa Scores", x = "Kappa", y = "Frequency") +
  theme_minimal()

```
inter-rater agreement in the word level: by word-level consensus
``` {r}

get_mode <- function(v) {
  uniq_v <- unique(na.omit(v))
  uniq_v[which.max(tabulate(match(v, uniq_v)))]
}


consensus_data <- full_data %>%
  group_by(block, theword) %>%
  summarise(
    mode_rating = get_mode(response_numeric),
    agreement_score = mean(response_numeric == mode_rating, na.rm = TRUE) * 100,
    sd_rating = sd(response_numeric, na.rm = TRUE)
  ) 


ggplot(consensus_data, aes(x = sd_rating, y = agreement_score, color = block)) +
  geom_point(size = 3, alpha = 0.7) +
  labs(title = "Agreement Score vs. Rating Standard Deviation", x = "Standard Deviation of Ratings", y = "Agreement Score (%)") +
  theme_minimal() +
  scale_color_brewer(palette = "Set3")

```
 incorporating rater count into the plot
```{r}

consensus_data <- full_data %>%
  group_by(block, theword) %>%
  summarise(
    mode_rating = get_mode(response_numeric),
    agreement_score = mean(response_numeric == mode_rating, na.rm = TRUE) * 100,
    sd_rating = sd(response_numeric, na.rm = TRUE),
    rater_count = n() 
  )


# rater count as point size
agreementword <- ggplot(consensus_data, aes(x = sd_rating, y = agreement_score, color = block, size = rater_count)) +
  geom_point(alpha = 0.7) +
  labs(
    title = "Agreement Score vs. Rating Standard Deviation with Rater Count",
    x = "Standard Deviation of Ratings",
    y = "Agreement Score (%)",
    size = "Rater Count"
  ) +
  theme_minimal() +
  scale_color_brewer(palette = "Set3")

ggsave("wordagreement.png", agreementword)

```

``` {r}
ggplot(consensus_data, aes(x = reorder(theword, -agreement_score), y = agreement_score, fill = block)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Agreement Score by Word and Block", x = "Word", y = "Agreement Score (%)") +
  coord_flip() +
  theme_minimal() +
  scale_fill_brewer(palette = "Set3") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(word_agreement, aes(x = agreement_score)) +
  geom_histogram(binwidth = 0.1, fill = "blue", color = "black") +
  labs(title = "Distribution of Word-Level Agreement Scores",
       x = "Agreement Score (SD of Ratings)",
       y = "Frequency")

```

# Venn diagram

Try to get a venn diagram-type representation. 

```{r}
threshold <- .7

category_thresholds <- category_counts |>
  filter(response %in% c("solid","count noun","shape")) |>
  mutate(over_threshold = proportion > threshold) |>
  select(-block, -totcount, -count, -proportion) |> 
  pivot_wider(names_from = "response", values_from = "over_threshold", 
              values_fill = FALSE) |>
  rename(count_noun = `count noun`)

category_overlap <- word_language_mappings |>
  left_join(category_thresholds) |>
  filter(!is.na(count_noun)) 
```

One example. 

```{r}
eng <- filter(category_overlap, language == "English (American)") |>
  select(-language) |>
  filter(!is.na(count_noun))
ggvenn(eng, show_elements = TRUE, c("count_noun","solid","shape"))
``` 
Lots of plots. 

```{r}
venns <- category_overlap |>
  group_by(language) |>
  nest() |>
  mutate(plot = map(data, ~ggvenn(.x, c("count_noun","solid","shape"),
                                  show_percentage = FALSE, 
                                  text_size = 2,
                                  set_name_size = 2)
                    + ggtitle(language))) |>
  pull(plot)

grid.arrange(grobs = venns, ncol = 4)

```
venn diagram: 
```{r}

# Define and generate the Venn diagrams with adjusted aesthetics
venns <- category_overlap |>
  group_by(language) |>
  nest() |>
  mutate(plot = map(data, ~ggvenn(.x, c("count_noun", "solid", "shape"),
                                  show_percentage = FALSE, 
                                  text_size = 7,       # Smaller text size for numbers
                                  set_name_size = 3) # Smaller text size for set names
                    + ggtitle(unique(language))    # Use language as the title
                    + theme(
                        plot.title = element_text(size = 10, hjust = 0.5),   # Adjust title size
                        plot.margin = margin(2, 2, 2, 2)                     # Adjust margins
                    ))) |>
  pull(plot)

# Define number of columns
ncol <- 4
# Dynamically determine number of rows
nrow <- ceiling(length(venns) / ncol)

# Arrange and save with specified layout
venn_grid <- grid.arrange(grobs = venns, ncol = ncol, nrow = nrow)
ggsave("venn_grid.png", venn_grid, width = 16, height = 16, dpi = 300)

```

``` {r}

# Group data by word and count_noun, shape, and solid properties, retaining languages where overlaps occur
overlap_df <- category_overlap %>%
  group_by(word, count_noun, shape, solid) %>%
  filter(n() > 1) %>%  # Only include words that overlap across languages
  ungroup()

# Expand rows so that each word-language combination appears separately
expanded_overlap_df <- overlap_df %>%
  select(word, count_noun, shape, solid, language) %>%
  distinct()  # Ensure unique word-language combinations

# Display the result
expanded_overlap_dfe

```

```{r}
inner_join(solid_df %>% filter(proportion <= 0.5), 
           count_df %>% filter(proportion <= 0.5), 
           by= "word") %>% 
  select(word) %>% 
  unique()

inner_join(solid_df %>% filter(proportion > 0.5), 
           count_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), count_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), count_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), shape_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), shape_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion <= 0.5), shape_df %>% filter(proportion > 0.5) , by= "word") %>% select(word) %>% unique()

inner_join(solid_df %>% filter(proportion > 0.5), shape_df %>% filter(proportion <= 0.5) , by= "word") %>% select(word) %>% unique()
```



``` {r }
dfs <- list(solid_df, count_df, shape_df)
names <- c("solid", "count", "shape")
conditions <- c("low", "high")

# Generate all unique pairs of dataframe and condition combinations
comparison_combinations <- expand.grid(
  df1 = names, df2 = names,
  prop1 = conditions, prop2 = conditions,
  stringsAsFactors = FALSE
) %>%
  filter(df1 != df2)  # no same dataframe comparisons

# Apply over each combination
results <- comparison_combinations %>%
  pmap_dfr(~ find_shared_words(
    dfs[[which(names == ..1)]],
    dfs[[which(names == ..2)]],
    ..1, ..2, ..3, ..4
  ))
 print(results)

results %>% group_by(source1, source2, condition1, condition2) %>% summarize(count = n())
```

# correspondence: uisng lms and correlations

``` {r }
correspodence_date <-  category_counts %>%
  filter(response %in% c("solid", "shape", "count noun")) %>%  # Filter only the desired responses
  select(word, response, proportion) %>%  # Keep only relevant columns
  pivot_wider(names_from = response, values_from = proportion, values_fill = 0)  # Reshape

summary(lm(data = correspodence_date, solid ~ `count noun`))
summary(lm(data = correspodence_date,  `count noun` ~ solid))

correspodence_date %>% select(`count noun`, shape, solid) %>% corr.test()



```

correlation
``` {r }
category_overlap %>% select(count_noun, shape, solid) %>% corr.test()

```
multivariarte model:
``` {r }
multivariable_model <- lm(shape ~ `count noun` + solid, data = correspodence_date)
summary(multivariable_model)

interaction_model <- lm(shape ~ `count noun` * solid, data = correspodence_date)
summary(interaction_model)

```


# more plot
randomly sample 50 words: 

``` {r}
randm_solid <- category_counts %>%
  filter(block == "solidity") %>% 
  {                               
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }

randm_syntax <- category_counts %>% filter(block == "count_mass") %>% 
  {
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }

randm_categ <- category_counts %>% filter(block == "category_organization") %>% 
  {
    sampled_words <- sample(unique(.$word), 50, replace = FALSE)
    filter(., word %in% sampled_words)
  }
```
reorder

```{r}

solid_proportions <- randm_solid %>%
  filter(response == "solid") %>%
  select(word, solid_proportion = proportion)

# merge the proportion of "solid" back into the main data frame
randm_solid <- randm_solid %>%
  left_join(solid_proportions, by = "word") %>%
  mutate(solid_proportion = replace_na(solid_proportion, 0))

# reorder 'word' based on 'solid_proportion'
randm_solid <- randm_solid %>% 
  mutate(word = fct_reorder(word, solid_proportion, .desc = TRUE))

#length(unique(languages$word))

```
```{r}
ggplot(randm_solid, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()

ggplot(randm_syntax, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()

ggplot(randm_categ, aes(x = word, y = proportion, fill = response)) +           geom_bar(position = "stack", stat = "identity") + coord_flip()
```

ordered

```{r}

ggplot(randm_solid, aes(x = word, y = proportion, fill = response)) +
  geom_bar(position = "stack", stat = "identity") +
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
# rotate x-axis text for better readability

```

#MA estimates
``` {r}
MA_estimates <- tibble(
  language = c("Mandarin (Beijing)", "English (American)", "Spanish (Mexican)", "Korean", "Japanese"),
estimates = c(0.8260, 1.0327,2.4976,1.682 ,1.1436), 
se = c(0.8389, 0.8477	, 1.1928	,  0.9035	, 0.9251)
  
)

MA_estimates <- left_join(MA_estimates, meansolid_perlanguage, by= "language")

ggplot(MA_estimates, aes(x = estimates.y, y = estimates.x, color = language)) +
  geom_point(size = 1) +
  geom_errorbar(aes(ymin = estimates.x - se, ymax = estimates.x + se), width = 0.001) + 

  labs(
    x = "Solidity Proportion",
    y = "Meta-analytic estimates"
  ) +
  theme_minimal()
```